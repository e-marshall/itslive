{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing ITS_LIVE data via S3 bucket\n",
    "\n",
    "This notebook will demonstrate how to access cloud-hosted Inter-mission Time Series of Land Ice Velocity and Elevation (ITS_LIVE) data from AWS S3 buckets. Here you will find examples of how to successfully access cloud-hosted data as well as some common errors and issues you may run into along the way, what they mean, and how to resolve them. \n",
    "\n",
    "*Learning goals:*\n",
    "- accessing data stored in s3 buckets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import cartopy\n",
    "import cartopy.feature as cfeature\n",
    "import json\n",
    "import s3fs\n",
    "\n",
    "%config InlineBackend.figure_format='retina'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ITS_LIVE data cube catalog\n",
    "\n",
    "The ITS_LIVE project details a number of data access options on their [website](https://its-live.jpl.nasa.gov/#access). Here, we will be accessing ITS_LIVE data in the form of `zarr` data cubes that are stored in **s3 buckets** hosted by Amazon Web Services (AWS). \n",
    "\n",
    "Let's begin by looking at the [GeoJSON Data Cubes Catalog](https://its-live-data.s3.amazonaws.com/datacubes/catalog_v02.json). \n",
    "This catalog contains spatial information and properties of ITS_LIVE data cubes as well as the url used to access each cube. Let's take a look at the entry for a single data cube and the information that it contains:\n",
    "\n",
    "![itslive_info](images/screengrab_itslive_catalog_entry.png)\n",
    "\n",
    "The top portion of the picture shows the spatial extent of the data cube in lat/lon units. Below that, we have properties such as the epsg code of the coordinate reference system, the spatial footprint in projected units and the url of the zarr object. \n",
    "\n",
    "Let's take a look at the url more in-depth: \n",
    "\n",
    "![itslive_url](images/itslive_url.png)\n",
    "\n",
    "From this link we can see that we are looking at its_live data located in an s3 bucket hosted by amazon AWS. We cans see that we're looking in the data cube directory and what seems to be version 2. The next bit gives us information about the global location of the cube (N40E080). The actual file name `ITS_LIVE_vel_EPSG32645_G0120_X250000_Y4750000.zarr` tells us that we are looking at ice velocity data (its_live also has elevation data), in the CRS associated with EPSG 32645 (this code indicates UTM zone 45N). X250000_Y4750000 tells us more about the spatial footprint of the datacube within the UTM zone. \n",
    "\n",
    "**NOTE**\n",
    "This catalog provides http links to the zarr objects. To successfully point to the objects that we're looking for in s3 buckets, we need to make a few changes to the links:\n",
    "- replace 'http' with 's3'\n",
    "- delete '.s3.amazonaws.com' </br>\n",
    "\n",
    "so the correct url should read: </br>\n",
    "\n",
    "    `s3://its-live-data/datacubes/v02/N40E080/ITS_LIVE_vel_EPSG32645_G0120_X250000_Y4750000.zarr`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing ITS_LIVE data from python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've found the url associated with the tile we want to access, let's try to open the data cube using `xarray`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "raises-exception",
     "output-scroll"
    ],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "url1 = 's3://its-live-data/datacubes/v02/N40E080/ITS_LIVE_vel_EPSG32645_G0120_X250000_Y4750000.zarr'\n",
    "dc1 = xr.open_dataset(url1, engine = 'zarr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this doesn't quite work. We need to specify a bit more information for xarray to be able to access and load the data cube. \n",
    "- **Describe a little more** Specifically we need to specify ``storage_options={\"anon\": True}``.\n",
    "- I set `chunks=\"auto\"` so that displaying the repr doesn't download data (which made it slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "dc1 = xr.open_dataset(url1, engine= 'zarr', chunks=\"auto\",\n",
    "                                    storage_options = {'anon':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "dc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one worked! Let's stop here and define a function that we can use for a quick inspection of this data.\n",
    "\n",
    "**This function uses cartopy for geographic plotting, geopandas for ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_bounds_polygon(input_xr):\n",
    "    \n",
    "    xmin = input_xr.coords['x'].data.min()\n",
    "    xmax = input_xr.coords['x'].data.max()\n",
    "\n",
    "    ymin = input_xr.coords['y'].data.min()\n",
    "    ymax = input_xr.coords['y'].data.max()\n",
    "\n",
    "    pts_ls = [(xmin, ymin), (xmax, ymin),(xmax, ymax), (xmin, ymax), (xmin, ymin)]\n",
    "\n",
    "    #print(input_xr.mapping.spatial_epsg)\n",
    "    #print(f\"epsg:{input_xr.mapping.spatial_epsg}\")\n",
    "    crs = f\"epsg:{input_xr.mapping.spatial_epsg}\"\n",
    "    #crs = {'init':f'epsg:{input_xr.mapping.spatial_epsg}'}\n",
    "    #crs = 'epsg:32645'\n",
    "    #print(crs)\n",
    "\n",
    "    polygon_geom = Polygon(pts_ls)\n",
    "    polygon = gpd.GeoDataFrame(index=[0], crs=crs, geometry=[polygon_geom]) \n",
    "    \n",
    "    return polygon\n",
    "    \n",
    "    \n",
    "def get_bbox_single(input_xr):\n",
    "    \n",
    "    '''Takes input xr object (from itslive data cube), plots a quick map of the footprint. \n",
    "    currently only working for granules in crs epsg 32645'''\n",
    "\n",
    "    \n",
    "    polygon = get_bounds_polygon(input_xr).to_crs('epsg:4326')\n",
    "    bounds = polygon.total_bounds\n",
    "    bounds_format = [bounds[0]-15, bounds[2]+15, bounds[1]-15, bounds[3]+15]\n",
    "\n",
    "    states_provinces = cfeature.NaturalEarthFeature(\n",
    "        category = 'cultural',\n",
    "        name = 'admin_1_states_provinces_lines',\n",
    "        scale='50m',\n",
    "        facecolor='none'\n",
    "    )\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection = ccrs.PlateCarree())\n",
    "    ax.stock_img()\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.LAND)\n",
    "    ax.add_feature(states_provinces)\n",
    "\n",
    "    ax.set_extent(bounds_format, crs = ccrs.PlateCarree())\n",
    "\n",
    "    polygon.plot(ax=ax, facecolor = 'none', edgecolor='red', lw=1.);\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also write a quick function for reading in s3 objects from http urls. This will come in handy when we're trying to test multiple urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def read_in_s3(http_url, chunks = 'auto'):\n",
    "    s3_url = http_url.replace('http','s3')\n",
    "    s3_url = s3_url.replace('.s3.amazonaws.com','')\n",
    "\n",
    "    datacube = xr.open_dataset(s3_url, engine = 'zarr',\n",
    "                                storage_options={'anon':True},\n",
    "                                chunks = chunks)\n",
    "\n",
    "    return datacube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the cube we've already read in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "get_bbox_single(dc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see where this granule lies. \n",
    "\n",
    "Let's try a url that we know won't work. Sometimes data cubes are moved around within the cloud repositories and it is helpful to know the error that arises in this situation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "\"output_scroll\"",
     "\"raises_exception\""
    ],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#this url doesn't work\n",
    "url2 = 'http://its-live-data.s3.amazonaws.com/datacubes/v02/wrong_url_here.zarr'\n",
    "dc2 = read_in_s3(url2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see we have a `NoSuchKey` error and then finally a `GroupNotFoundError`, these tell us that the issue with the code in the above cell was that the url was pointing to a location in the s3 bucket that did not contain the specified data cube. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching ITS_LIVE catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how we could search the ITS_LIVE data cube catalog for the data that we're interested in. There are many ways to do this, this is just one example. \n",
    "\n",
    "First, we will read in the catalog geojson file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "with urllib.request.urlopen('https://its-live-data.s3.amazonaws.com/datacubes/catalog_v02.json') as url:\n",
    "    itslive_catalog = json.loads(url.read().decode())\n",
    "itslive_catalog.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python, the json object has the form of nested dictionaries that contain information about all of the its_live datacubes. \n",
    "\n",
    "Here we'll show two options for filtering the catalog: \n",
    "1. one for selecting granules that contain a specific point, and \n",
    "2. one that returns all granules within a single UTM zone (specified by epsg code).  This will let us take stock of the spatial coverage of data cubes located at **working urls** within a certain UTM zone. \n",
    "\n",
    "You could easily tweak these functions (or write your own!) to select granules based on different properties. Play around with the `itslive_catalog` object to become more familiar with the data it contains and different options for indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting data cubes by UTM zone\n",
    "\n",
    "\n",
    "Is the cell below used?\n",
    "\n",
    "**Describe s3fs**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import s3fs\n",
    "fs  = s3fs.S3FileSystem(anon=True)\n",
    "fs\n",
    "\n",
    "#fs.lexists(s3_url_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def find_granules_by_zone(input_dict, epsg_code):\n",
    "    '''This function takes a dictionary (itslive catalog geojson) and a epsg code referencing region of interest. \n",
    "    returns list of urls corresponding to datacubes stored in s3 buckets where links *exist*'''\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "    #url_ls = []\n",
    "    for granule in input_dict['features']:\n",
    "        props = granule['properties']\n",
    "        if props['data_epsg'] == epsg_code:\n",
    "            http_url = props['zarr_url']\n",
    "            s3_url = http_url.replace('http','s3').replace('.s3.amazonaws.com','')\n",
    "\n",
    "            # check that the url works.\n",
    "            urls_ls = [s3_url if fs.lexists(s3_url) == True]\n",
    "            #if fs.lexists(s3_url) == True:\n",
    "            #    url_ls.append(s3_url)\n",
    "\n",
    "    return url_ls\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "output-scroll"
    ],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "granule_ls_32645 = find_granules_by_zone(itslive_catalog, 'EPSG:32645')\n",
    "granule_ls_32645[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to look at the spatial extent of all of the data cubes pointed to in the above list, we could use the `get_bbox_group()` function, but we'd first need to read in the data cubes at the urls as `xr.Datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "dc_ls = [read_in_s3(x) for x in granule_ls_32645]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns a list of xarray objects that correspond to all of the zarr urls for that region that were able to be successfully accessed.\n",
    "\n",
    "Let's write a function to visualize the coverage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_bbox_group(input_ls, bounds = [-180, 180, -90, 90]): \n",
    "    \n",
    "    '''plots the spatial extents of a list of datacubes'''\n",
    "    \n",
    "    poly_ls = [get_bounds_polygon(xr_obj).to_crs('epsg:32645') for xr_obj in input_ls]\n",
    "    #poly_ls = []\n",
    "    \n",
    "    #for xr_obj in input_ls:\n",
    "    #    polygon = get_bounds_polygon(xr_obj).to_crs('epsg:4326')\n",
    "    #    poly_ls.append(polygon)\n",
    "        \n",
    "    ### alternatively use list comprehension\n",
    "    # poly_ls = [get_bounds_polygon(xr_obj).to_crs('epsg:4326') for xr_obj in input_ls]\n",
    "\n",
    "    bounds_format = [bounds[0], bounds[1], bounds[2], bounds[3]]\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection = ccrs.PlateCarree())\n",
    "    ax.stock_img()\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.LAND)\n",
    "    ax.add_feature(cfeature.BORDERS)\n",
    "    ax.set_extent(bounds_format, crs = ccrs.PlateCarree())\n",
    "    \n",
    "    gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                  linewidth=2, color='gray', alpha=0.5, linestyle='--')\n",
    "    gl.top_labels = False\n",
    "    gl.left_labels = False\n",
    "    gl.xlines = False\n",
    "    gl.xlocator = mticker.FixedLocator([-180, -45, 0, 45, 180])\n",
    "    gl.xformatter = LONGITUDE_FORMATTER\n",
    "    gl.yformatter = LATITUDE_FORMATTER\n",
    "    gl.xlabel_style = {'size': 15, 'color': 'gray'}\n",
    "    gl.xlabel_style = {'color': 'black'}\n",
    "\n",
    "    for polygon in poly_ls:\n",
    "            \n",
    "        #polygon.plot(ax=ax, facecolor = 'none', edgecolor='red', lw=1.)\n",
    "        polygon.plot(ax=ax, facecolor='none', edgecolor='red', lw=1.);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "get_bbox_group(dc_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the region of interest, we can do that by specifying bounds when we call the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "get_bbox_group(dc_ls, bounds=[70, 98, 20, 60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting granules by a single point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def find_granule_by_point(input_dict, input_point): #[lon,lat]\n",
    "    '''Takes an inputu dictionary (a geojson catalog) and a point to represent AOI.\n",
    "    this returns a list of the s3 urls corresponding to zarr datacubes whose footprint covers the AOI'''\n",
    "    #print([input_points][0])\n",
    "    \n",
    "    target_granule_urls = []\n",
    "    #Point(coord[0], coord[1])\n",
    "    #print(input_point[0])\n",
    "    #print(input_point[1])\n",
    "    point_geom = Point(input_point[0], input_point[1])\n",
    "    #print(point_geom)\n",
    "    point_gdf = gpd.GeoDataFrame(crs='epsg:4326', geometry = [point_geom])\n",
    "    for granule in input_dict['features']:\n",
    "        \n",
    "        #print('tick')\n",
    "        bbox_ls = granule['geometry']['coordinates'][0]\n",
    "        bbox_geom = Polygon(bbox_ls)\n",
    "        bbox_gdf = gpd.GeoDataFrame(index=[0], crs='epsg:4326', geometry = [bbox_geom])\n",
    "        \n",
    "        #if poly_gdf.contains(points1_ls[poly]).all() == True:\n",
    "\n",
    "        if bbox_gdf.contains(point_gdf).all() == True:\n",
    "            #print('yes')\n",
    "            target_granule_urls.append(granule['properties']['zarr_url'])\n",
    "        else:\n",
    "            pass\n",
    "            #print('no')\n",
    "    return target_granule_urls\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "url_ls = find_granule_by_point(itslive_catalog, [86.7, 28.07])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, this function returned a single url corresponding to the data cube covering the point we supplied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "url_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `read_in_s3` function we defined to open the datacube as an `xarray.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "dc1 = read_in_s3(url_ls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then the `get_bbox_single` function to take a look at the footprint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "get_bbox_single(dc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we know how to access the its_live data cubes for a given region as well as at a specific point.\n",
    "\n",
    "Let's take a quick first look at this data cube. The next notebooks will go into more examples of inspecting and working with this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "dc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "dc1.v.mean(dim = 'mid_date').plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows the mean of the magnitude of velocity variable along the time dimension. It looks like the footprint we're viewing has glaciated terrain in the southern region as well as non-glaciated terrain in the northern region. The rest of the notebooks in this chapter will demonstrate how to subset this spatially-large data object to specific areas of interest."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "mynewbook",
   "language": "python",
   "name": "mynewbook"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
