{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9134a9e",
   "metadata": {},
   "source": [
    "# Region-scale glacier analysis\n",
    "\n",
    "\n",
    "The previous notebook demonstrated using xarray to analyze surface velocity data for an individual glacier. This notebook will show how we can examine spatial variability in surface velocity within a group of glaciers. To do this we will use xarray as well as **geopandas**, **geocube**, and **pandas**. We will start by using `make_geocube()` to rasterize a vector object in the shape of an **ITS_LIVE** velocity raster object. We will then use the rasterized vector to group the **ITS_LIVE** object by individual glaciers and then calculate summary statistics of surface velocity for each glacier. The goal in this work flow is to end up with a **pandas dataframe** where each row is an individual glacier and columns for various surface velocity summary statistics. \n",
    "\n",
    "*Learning goals*\n",
    "- rasterizing vector data\n",
    "- organizing and re-arranging data with xarray\n",
    "- `groupby()` for zonal statistics\n",
    "- converting from xarray to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41dc04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import cartopy\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "from geocube.api.core import make_geocube\n",
    "\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d2819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itslivetools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a61c98-901e-425d-ad6c-4627b92bf6df",
   "metadata": {},
   "source": [
    "## Accessing ITS_LIVE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d209e9c5-3849-4100-9ac0-49e0dda09659",
   "metadata": {},
   "outputs": [],
   "source": [
    "with urllib.request.urlopen('https://its-live-data.s3.amazonaws.com/datacubes/catalog_v02.json') as url_catalog:\n",
    "    itslive_catalog = json.loads(url_catalog.read().decode())\n",
    "itslive_catalog.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4942a00-751c-4ffa-b8c5-56d0ae07fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = itslivetools.find_granule_by_point(itslive_catalog, [84.56, 28.54])\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f56626a-13cc-4823-a8de-21293366e46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = itslivetools.read_in_s3(url[0])\n",
    "dc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55796ca1",
   "metadata": {},
   "source": [
    "The `mid_date` dimension of the `dc` object isn't in chronlogical order, so let's sort by this dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa3d48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = dc.sortby('mid_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1957308",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9100f674",
   "metadata": {},
   "source": [
    "## Vector data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb739146",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_asia = gpd.read_file('https://github.com/scottyhq/rgi/raw/main/15_rgi60_SouthAsiaEast.gpkg')\n",
    "se_asia.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d105f141",
   "metadata": {},
   "source": [
    "How many glaciers are in this dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b92444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_asia['RGIId'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aaa855",
   "metadata": {},
   "source": [
    "What coordinate reference system is this dataframe in? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f54c1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_asia.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f616aad9",
   "metadata": {},
   "source": [
    "The vector dataset is in WGS 84, meaning that its coordinates are in degrees latitude and longitude rather than meters N and E. We will project this dataset to match the projection of the netcdf dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9183f2ce",
   "metadata": {},
   "source": [
    "## Handling projections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d94b63",
   "metadata": {},
   "source": [
    "Let's project this dataframe to match the CRS of the itslive dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd69a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#project rgi data to match itslive\n",
    "se_asia_prj = se_asia.to_crs('EPSG:32645') #we know the epsg from looking at the 'spatial epsg' attr of the mapping var of the dc object\n",
    "se_asia_prj.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fd3ab5",
   "metadata": {},
   "source": [
    "Give each glacier (row) a unique integer key that is related to that glacier's RGIId. We will use this later. Be careful that the `RGI_int` column is composed of **integers** not strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2bd49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_asia_prj['RGI_int'] = se_asia_prj['RGIId'].str.slice(9,).replace('.','_')\n",
    "se_asia_prj['RGI_int'] = se_asia_prj.RGI_int.apply(lambda x: int('15' + x))\n",
    "se_asia_prj.RGI_int.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723e1db7",
   "metadata": {},
   "source": [
    "To start with, we will look only at glaciers larger in area than 5km2. Subset the dataset to select for those glaciers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee71a5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_asia_prj = se_asia_prj.loc[se_asia_prj['Area'] > 5.]\n",
    "se_asia_prj.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b505095",
   "metadata": {},
   "source": [
    "Next, want to subset the RGI dataset by the spatial extent of the ITS_LIVE data.\n",
    "First, get the bbox of the ITS_LIVE data as a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a530d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_bbox = itslivetools.get_bbox_single(dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5355bce9",
   "metadata": {},
   "source": [
    "Project it to local UTM to match the RGI geodataframe and extract the coordinate values from the geometry column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9b597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_bbox_prj = dc_bbox.to_crs('EPSG:32645')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f1619",
   "metadata": {},
   "source": [
    "Subset RGI dataset:\n",
    "To do this we will use a [spatial join](https://geopandas.org/en/stable/docs/reference/api/geopandas.sjoin.html). Here we use an `inner join` but there are various methods to customize the spatial join operation. Find more info [here](https://geopandas.org/en/stable/gallery/spatial_joins.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e3ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_sub = gpd.sjoin(se_asia_prj, dc_bbox_prj, how='inner')\n",
    "# need to set the type as string here bc for some reason its object intead of str\n",
    "rgi_sub[\"RGIId\"] = rgi_sub.RGIId.astype(\"string\")\n",
    "rgi_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eb41e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_sub['RGIId'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5afc5ad",
   "metadata": {},
   "source": [
    "Need to write crs of dc object?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0c495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = dc.rio.write_crs(f\"epsg:{dc.mapping.attrs['spatial_epsg']}\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e070e4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_sub = rgi_sub.drop('index_right', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047e0d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48db6227",
   "metadata": {},
   "source": [
    "Now, use the `make_geocube()` function. This essentially takes a vector object (`rgi_sub`) and rasterizes it, returning an xarray object with the same structure as the object you provide for the `like =` argument (in our case that is `dc`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f292aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_grid = make_geocube(\n",
    "    vector_data = rgi_sub,\n",
    "    measurements = [\"RGI_int\"],\n",
    "    like = dc\n",
    ")\n",
    "out_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c932b36",
   "metadata": {},
   "source": [
    "Now each glacier in the geodataframe `rgi_sub` has been coded with a unique integer value that corresponds to that glacier's Randolph Glacier Inventory ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5136c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_grid.RGI_int.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cbacf9",
   "metadata": {},
   "source": [
    "Next, merge the rasterized vector and the dataset containing the velocity data into an xarray dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa16f025",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_grid['v'] = dc.v \n",
    "out_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db689c58",
   "metadata": {},
   "source": [
    "Since we are mostly interested in examining spatial variability, let's take a temporal subset of the dataset to make the computation faster: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71948c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_grid_sub = out_grid.sel(mid_date = slice('2015-01-01','2015-02-01')).compute()\n",
    "out_grid_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbb0f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_ID = out_grid_sub.drop('spatial_ref').groupby(out_grid_sub['RGI_int'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb665162",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute zonal stats groupedd by ID\n",
    "grid_mean_sp = grouped_ID.mean(dim=[...]).rename({'v': 'speed_mean'})\n",
    "grid_min_sp = grouped_ID.min(dim=[...]).rename({'v': 'speed_min'})\n",
    "grid_max_sp = grouped_ID.max(dim=[...]).rename({'v': 'speed_max'})\n",
    "#grid_std_sp = grouped_ID.std(dim=['mid_date','stacked_y_x']).rename({'v': 'speed_std'}).compute()\n",
    "    \n",
    "#merge each zonal stat xr obj into a single xr obj, convert to pandas df\n",
    "#zonal_stats = xr.merge([grid_mean_sp, grid_min_sp, grid_max_sp, grid_std_sp]).to_dataframe()\n",
    "#zonal_stats = zonal_stats.reset_index()\n",
    "#zonal_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a8ef55",
   "metadata": {},
   "source": [
    "Check if the data arrays are equal (the RGI_ints of each should be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444bfafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_mean_sp.RGI_int.equals([grid_max_sp.RGI_int, grid_min_sp.RGI_int, grid_std_sp.RGI_int])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6096fe",
   "metadata": {},
   "source": [
    "Looks like the issue is with `grid_std_sp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1c1c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_mean_sp.RGI_int.equals(grid_std_sp.RGI_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81a15c5",
   "metadata": {},
   "source": [
    "Try to find the differences, ** stuck on this part.... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b07cc7",
   "metadata": {},
   "source": [
    "or.... could just not use std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c569a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge each zonal stat xr obj into a single xr obj, convert to pandas df\n",
    "zonal_stats = xr.merge([grid_mean_sp, grid_min_sp, grid_max_sp]).to_dataframe()\n",
    "zonal_stats = zonal_stats.reset_index()\n",
    "zonal_stats = zonal_stats.drop(['mapping','spatial_ref'], axis=1)\n",
    "zonal_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f52a93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_itslive = rgi_sub.loc[rgi_sub['Area'] > 5.].merge(zonal_stats, on='RGI_int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d38148",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_itslive.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc3800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(rgi_itslive['RGIId']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b937f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "rgi_itslive.plot.scatter(x='Aspect',y = 'speed_mean', c = 'darkblue', ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbfa1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_itslive.plot(column='speed_mean', legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308b86db-1a81-4d40-8f9b-19e6bb259f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_itslive.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49bf597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
