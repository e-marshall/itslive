{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual glacier data inspection\n",
    "\n",
    "This notebook will walk through steps to read in and organize velocity data and clip it to the extent of a single glacier. The tools we will use include **xarray**, **rioxarray**, **geopandas**, and **flox**. \n",
    "\n",
    "To clip its_live data to the extent of a single glacier we will use a vector dataset of glacier outlines, the [Randolph Glacier Inventory](https://nsidc.org/data/nsidc-0770). These aren't cloud-hosted currently so you will need to download the data to your local machine. \n",
    "\n",
    "*Learning goals*\n",
    "\n",
    "- subset large raster to spatial area of interest\n",
    "- exploring with **dask** and **xarray**\n",
    "- dataset inspection using\n",
    "    - xarray label and index-based selections\n",
    "    - grouped computations and reductions\n",
    "    - visualization\n",
    "\n",
    "First, lets install the python libraries that we'll need for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import cartopy\n",
    "import cartopy.feature as cfeature\n",
    "import json\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import flox\n",
    "%config InlineBackend.figure_format='retina'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in ITS_LIVE data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use some of the functions we defined in the data access notebook in this notebook and others within this tutorial. They will all be within the `itslivetools` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itslivetools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's read in the catalog again:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with urllib.request.urlopen('https://its-live-data.s3.amazonaws.com/datacubes/catalog_v02.json') as url_catalog:\n",
    "    itslive_catalog = json.loads(url_catalog.read().decode())\n",
    "itslive_catalog.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `read_in_s3()` function will read in a xarray dataset from a url to a zarr datacube when we're ready:\n",
    "\n",
    "I started with `chunk_size='auto'` which will choose chunk sizes that match the underlying data structure (this is generally ideal). More about choosing good chunk sizes [here](https://blog.dask.org/2021/11/02/choosing-dask-chunk-sizes). If you want to use a different chunk size, specify it when you call the `read_in_s3()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = itslivetools.find_granule_by_point(itslive_catalog, [84.56, 28.54])\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = itslivetools.read_in_s3(url[0])\n",
    "dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are reading this in as a dask array. Let's take a look at the chunk sizes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note} \n",
    "chunksizes shows the largest chunk size. chunks shows the sizes of all chunks along all dims, better if you have irregular chunks\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.chunksizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note} \n",
    "Setting the dask chunksize to `auto` at the `xr.open_dataset()` step will use chunk sizes that most closely resemble the structure of the underlying data. To avoid imposing a chunk size that isn't a good fit for the data, avoid re-chunking until we have selected a subset of our area of interest from the larger dataset \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check CRS of xr object: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the time dimension (`mid_date` here). To start with we'll just print the first 10 values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in range(10):\n",
    "    \n",
    "    print(dc.mid_date[element].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird, it doesn't look like the time dimension is in chronological order, let's fix that: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_timesorted = dc.sortby(dc['mid_date'])\n",
    "dc_timesorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we read in the zarr datacube as a `xr.Dataset` we set the chunk sizes to `auto`. When we try to sort along the `mid_date` dimension this seems to become a problem and we get the warning above. \n",
    "\n",
    "At first it makes sense to follow the instructions in the warning message to avoid creating large chunks, but this creates some issues. If you want, turn the cell below to `code` and run it, you can see that this re-chunks the time dimension which isn't something that we want "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import dask\n",
    "with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "    dc_timesorted_false = dc.sortby(dc['mid_date'])\n",
    "    dc_timesorted_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_timesorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in range(10):\n",
    "    \n",
    "    print(dc_timesorted.mid_date[element].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in vector data \n",
    "\n",
    "We are going to read in RGI region **15 (SouthAsiaEast)**. RGI data is downloaded in lat/lon coordinates. We will project it to match the CRS of the ITS_LIVE dataset and then select an individual glacier to begin our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_asia = gpd.read_file('/Users/emarshall/Desktop/siparcs/data/nsidc0770_15.rgi60.SouthAsiaEast/15_rgi60_SouthAsiaEast.shp')\n",
    "se_asia.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project rgi data to match itslive\n",
    "#we know the epsg from looking at the 'spatial epsg' attr of the mapping var of the dc object\n",
    "se_asia_prj = se_asia.to_crs('EPSG:32645') \n",
    "se_asia_prj.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop RGI to ITS_LIVE extent\n",
    "- is there a way to call `get_bbox_single()` without the plot output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, get vector bbox of itslive\n",
    "\n",
    "bbox_dc = itslivetools.get_bbox_single(dc)\n",
    "bbox_dc['geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project from latlon to local utm \n",
    "bbox_dc = bbox_dc.to_crs('EPSG:32645')\n",
    "bbox_dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset rgi to bounds \n",
    "se_asia_subset = gpd.clip(se_asia_prj, bbox_dc)\n",
    "se_asia_subset\n",
    "se_asia_subset.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_glacier_vec = se_asia_subset.loc[se_asia_subset['RGIId'] == 'RGI60-15.04714']\n",
    "sample_glacier_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip ITS_LIVE dataset to individual glacier extent\n",
    "\n",
    "First, we need to use rio.write_crs() to assign a CRS to the itslive object. If we don't do that first the `rio.clip()` command will produce an error\n",
    "*Note*: it looks like you can only run write_crs() once, because it switches mapping from being a `data_var` to a `coord` so if you run it again it will produce a key error looking for a var that doesnt' exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_timesorted = dc_timesorted.rio.write_crs(f\"epsg:{dc_timesorted.mapping.attrs['spatial_epsg']}\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sample_glacier_raster = dc_timesorted.rio.clip(sample_glacier_vec.geometry, sample_glacier_vec.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the clipped object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_glacier_raster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the clipped raster alongside the vector outline. To start with and for the sake of easy visualizing we will take the mean of the magnitude of velocity variable along the `mid_date` dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15,9))\n",
    "sample_glacier_vec.plot(ax=ax, facecolor='none', edgecolor='red');\n",
    "sample_glacier_raster.v.mean(dim=['mid_date']).plot(ax=ax);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the x and y components of velocity, again averaging over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols =2, figsize=(17,7))\n",
    "\n",
    "sample_glacier_raster.vx.mean(dim='mid_date').plot(ax=axs[0]);\n",
    "sample_glacier_raster.vy.mean(dim='mid_date').plot(ax=axs[1]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_glacier_raster.v_error.mean(dim=['mid_date']).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring ITS_LIVE data\n",
    "\n",
    "ITS_LIVE data cubes come with many (53!) variables that carry information about the estimated surface velocities and the satellite images that were used to generate the surface velocity estimates. We won't examine all of this information here but let's look at a litte bit.\n",
    "\n",
    "To start with, let's look at the satellite imagery used to generate the velocity data.\n",
    "\n",
    "We see that we have two `data_vars` that indicate which sensor that each image in the image pair at a certain time step comes from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_glacier_raster.satellite_img1.data.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_glacier_raster.satellite_img2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `satellite_img1` and `satellite_img2` variables are 1-dimensional numpy arrays corresponding to the length of the `mid_date` dimension of the data cube. You can see that each element of the array is a string corresponding to a different satellite:\n",
    "    `1A` = Sentinel 1A, `1B` = Sentinel 1B, `2A` = Sentinel 2A\n",
    "    `2B` = Sentinel 2B, `8.` = Landsat8 and `9.` = Landsat9\n",
    "    \n",
    "Let's re-arrange these string arrays into a format that is easier to work with.\n",
    "\n",
    "First, we'll make a set of all the different string values in the satellite image variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining velocity data from each satellite in `ITS_LIVE` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we only wanted to look at the velocity estimates from landat8?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l8_data = sample_glacier_raster.where(sample_glacier_raster['satellite_img1'] == '8.', drop=True)\n",
    "l8_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dataset.where()` at first seems appropriate to use for kind of operation but there's actually an easier way. Because we are selecting along a single dimension (`mid_date`), we can use xarray's `.sel()` method instead. This is more efficient and integrates with `dask` arrays more smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l8_condition = sample_glacier_raster.satellite_img1.isin('8.')\n",
    "l8_subset = sample_glacier_raster.sel(mid_date=l8_condition)\n",
    "l8_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we are looking at roughly a third of the original time steps. Let's take a look at the average speeds of the Landsat8-derived velocities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l8_subset.v.mean(dim='mid_date').plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about Landsat9?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l9_condition = sample_glacier_raster.satellite_img1.isin('9.')\n",
    "\n",
    "l9_subset = sample_glacier_raster.sel(mid_date=l9_condition)\n",
    "l9_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 45 time steps have data from Landsat9, this makes sense because Landsat9 was just launched recently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l9_subset.v.mean(dim='mid_date').plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at Sentinel 1 data. Note here we are selecting for 2 values instead of 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_condition = sample_glacier_raster.satellite_img1.isin(['1A','1B'])\n",
    "s1_subset = sample_glacier_raster.sel(mid_date = s1_condition)\n",
    "s1_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_subset.v.mean(dim='mid_date').plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_condition = sample_glacier_raster.satellite_img1.isin(['2A','2B'])\n",
    "s2_subset = sample_glacier_raster.sel(mid_date=s2_condition)\n",
    "s2_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_subset.v.mean(dim='mid_date').plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ITS_LIVE is exciting because it combines velocity data from a number of satellites into one accessible and efficient dataset. From this brief look, you can see snapshot overviews of the different data within the dataset and begin to think about processing steps you might take to work with the data further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking coverage along a dimension\n",
    "It would be nice to be able to scan/visualize and observe coverage of a variable along a dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First need to make a mask that will tell us all the possible 'valid' pixels. ie pixels over ice v. rock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pixels = sample_glacier_raster.v.count(dim=['x','y']).compute()\n",
    "valid_pix_max = sample_glacier_raster.v.notnull().any('mid_date').sum(['x','y'])\n",
    "\n",
    "cov = valid_pixels/valid_pix_max\n",
    "\n",
    "sample_glacier_raster['cov'] = cov\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking into `mid_dates` dimension...\n",
    "\n",
    "- how many duplicate time steps are there? \n",
    "\n",
    "My trouble shooting steps... keeping in for now but can delete when ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many time steps are duplicates?, there are 16872 unique vals in mid_dates\n",
    "np.unique(sample_glacier_raster['mid_date'].data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by grouping over `mid_date`. Would expect 16,872 (# unique time steps) with mostly groups of 1, groups of more than one on duplicate coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gb = sample_glacier_raster.groupby(sample_glacier_raster.mid_date)\n",
    "type(test_gb.groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`test_gb.groups` is a dict, so let's explore that object. the keys correspond to `mid_date` coords, so the values should be the entries at that coordinate. Want to find dict entries with more than one value..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_ls = [len(val) for val in test_gb.groups.values()] #this is hopefully a list of the number of vals in each dict key\n",
    "\n",
    "val_df = pd.DataFrame({'num_vals': val_ls})\n",
    "val_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset the values dataframe to only keep rows with more than one value per key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_sub = val_df.loc[val_df['num_vals'] > 1]\n",
    "val_df_sub.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, df is 2602 rows (# time steps with more than one entry). How many have more than 2? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_sub.plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one of the time steps with multiple entries. Used a for loop fn on the first year of the dataset for this (below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_middate = sample_glacier_raster.sel(mid_date = '2013-09-30T04:56:01.528083968').compute()\n",
    "duplicate_middate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mid date: ', duplicate_middate.mid_date.mid_date.data)\n",
    "print('image 1 date for entries 1 and 2: ', duplicate_middate.acquisition_date_img1.data)\n",
    "print('image 2 date for entries 1 and 2: ', duplicate_middate.acquisition_date_img2.data)\n",
    "\n",
    "time_diff1 = duplicate_middate.acquisition_date_img1.data[0] - duplicate_middate.acquisition_date_img2.data[0]\n",
    "\n",
    "diff_days1 = time_diff1.astype('timedelta64[D]')\n",
    "print(diff_days1/np.timedelta64(1,'D'), ' days')\n",
    "\n",
    "time_diff2 = duplicate_middate.acquisition_date_img1.data[1] - duplicate_middate.acquisition_date_img2.data[1]\n",
    "diff_days2 = time_diff2.astype('timedelta64[D]')\n",
    "print(diff_days2/np.timedelta64(1,'D'), ' days')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, looks like both entries are Landsat8 velocity data, one generated from image pairs 16 days apart and one from image pair 48 days apart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dim_duplicates(input_xr):\n",
    "\n",
    "    for element in range(len(input_xr.mid_date)):\n",
    "        if (element+1) <= 284:\n",
    "            if input_xr.isel(mid_date=element).mid_date.data == input_xr.isel(mid_date=(element+1)).mid_date.data:\n",
    "                print(input_xr.isel(mid_date=element).mid_date.data)\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_dim_duplicates(sample_glacier_raster.sel(mid_date = slice('2013-01-01','2014-01-01')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring data coverage over time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data coverage over this glacier across the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(30,3))\n",
    "sample_glacier_raster.cov.plot(ax=ax, linestyle='None',marker = 'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we wanted to explore the relative coverage of the different sensors that make up the its_live dataset as a whole?\n",
    "We can use `groupby` to group the data based on a single condition such as `satellite_img1` or `mid_date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_glacier_raster.cov.groupby(sample_glacier_raster.satellite_img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_glacier_raster.groupby('mid_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we want to examine the coverage of data from different sensor groups over time, we would essentially want to `groupby` two groups. To do this, we use `flox`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flox.xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the `xr.DataArray` on which we will perform the grouping operation using `flox`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_glacier_raster.cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `flox`, we will define a coverage object that takes as inputs the data we want to reduce, the groups we want to use to group the data and the reduction we want to perform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = flox.xarray.xarray_reduce(\n",
    "    sample_glacier_raster.cov,\n",
    "    sample_glacier_raster.satellite_img1.compute(),\n",
    "    sample_glacier_raster.mid_date,\n",
    "    func=\"mean\",\n",
    "    fill_value=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize the coverage over time for each sensor in the its_live dataset. Cool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(coverage.mid_date, coverage.satellite_img1, coverage, cmap='viridis')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook displayed basic data inspection steps that you can take when working with a new dataset. The following notebooks will demonstrate further processing, analytical and visualization steps you can take. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env-itslivetools-py",
   "language": "python",
   "name": "conda-env-itslivetools-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
